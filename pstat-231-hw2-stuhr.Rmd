---
title: 'Homework #2'
author: "Jaxon Stuhr"
date: "2022-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(yardstick)
library(ggthemes)
library(here)
```


Load in the abalone dataset
```{r}
abalone_raw = read_csv(here("data", "abalone.csv"))
set.seed(23)
```

## Question 1
Your goal is to predict abalone age, which is calculated as the number of rings plus 1.5. Notice there currently is no age variable in the data set. Add age to the data set.

Assess and describe the distribution of age.

```{r}
abalone = abalone_raw %>% 
# add age variable to data
  mutate(age = rings + 1.5) %>% 
  select(-rings)

par(mfrow=c(1,2))

ggplot(abalone, aes(x = age)) +
  geom_histogram()

ggplot(abalone, aes(sample = age)) +
  stat_qq() +
  stat_qq_line(lwd = 1)

mean(abalone$age)
median(abalone$age)
```

The distribution of age appears relatively normal around a median of 10.5 years, however, there is an exceptionally large tail of very old abalone, which brings the mean age up to 11.4 years, and appears highly non-normal in its distribution. 

## Question 2

Split the abalone data into a training set and a testing set. Use stratified sampling. You should decide on appropriate percentages for splitting the data.

```{r}
abalone_split <- initial_split(abalone, prop = 0.75,
                                strata = age)

abalone_train <- training(abalone_split)
abalone_test <- testing(abalone_split)
```

## Question 3

Using the training data, create a recipe predicting the outcome variable, age, with all other predictor variables. Note that you should not include rings to predict age. Explain why you shouldn’t use rings to predict age.

We should not use rings to predict age because our definition of age is directly based on the number of rings. We want to understand weather the age of abalone can be predicted from more readily available indicators.

```{r}
# generate recipe age as a function of all predictors
abalone_recipe <- recipe(age ~ ., data = abalone_train) %>% 
  # dummy vars for nominal predictors
  step_dummy(type) %>% 
  # set up interaction terms
  step_interact(terms = ~ starts_with("type"):shucked_weight) %>% 
  step_interact(terms = ~ longest_shell:diameter) %>% 
  step_interact(terms = ~ shucked_weight:shell_weight) %>%
  # center
  step_center(all_numeric_predictors()) %>%
  # scale
  step_scale(all_numeric_predictors())
```


## Question 4

Create and store a linear regression object using the "lm" engine.

```{r}
lm_model <- linear_reg() %>% 
  set_engine("lm")
```

## Question 5

Now:

1. set up an empty workflow,

2. add the model you created in Question 4, and

3. add the recipe that you created in Question 3.

```{r}
# set up empty workflow
lm_wflow <- workflow() %>% 
  # add model
  add_model(lm_model) %>% 
  # add recipe
  add_recipe(abalone_recipe)
```

## Question 6

Use your fit() object to predict the age of a hypothetical female abalone with longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, shell_weight = 1.

```{r}
lm_fit <- fit(lm_wflow, abalone_train)

sample_abalone = abalone[0,] %>% 
  select(-age) %>% 
  add_row(type = "F",  longest_shell = 0.50, diameter = 0.10, height = 0.30, whole_weight = 4, shucked_weight = 1, viscera_weight = 2, shell_weight = 1)

sample_prediction <- predict(lm_fit, new_data = sample_abalone)
```

Predicted Age: 24.8 years

## Question 7

Now you want to assess your model’s performance. To do this, use the yardstick package:

1. Create a metric set that includes R2, RMSE (root mean squared error), and MAE (mean absolute error).

2. Use predict() and bind_cols() to create a tibble of your model’s predicted values from the training data along with the actual observed ages (these are needed to assess your model’s performance).

3. Finally, apply your metric set to the tibble, report the results, and interpret the R2 value.

```{r}
# build metric set
abalone_metrics <- metric_set(rsq, rmse, mae)
# predict ages for all data in training set
abalone_train_ages <- predict(lm_fit, new_data = abalone_train %>% select(-age))
# add original ages to data
abalone_train_ages <- bind_cols(abalone_train_ages, abalone_train %>% select(age))
# apply metrics
abalone_metrics(abalone_train_ages, truth = age, 
                estimate = .pred)
```

RMSE = 2.18, mae = 1.566, , $R^2$ = .55

This means that the model as able to explain 55% of the variation in age from the predicted variables, or that 45% of the variation was not well predicted by the model. It did not do a very good job.

# Questions 8-10

In lecture, we presented the general bias-variance tradeoff, which takes the form:

$$
E[(y_0 - \hat{f}(x_0))^2]=Var(\hat{f}(x_0))+[Bias(\hat{f}(x_0))]^2+Var(\epsilon)
$$

where the underlying model $Y=f(X)+\epsilon$ satisfies the following:

- $\epsilon$ is a zero-mean random noise term and $X$ is non-random (all randomness in $Y$ comes from $\epsilon$);
- $(x_0, y_0)$ represents a test observation, independent of the training set, drawn from the same model;
- $\hat{f}(.)$ is the estimate of $f$ obtained from the training set.

## Question 8
Which term(s) in the bias-variance tradeoff above represent the reproducible error? Which term(s) represent the irreducible error?

The variance ($Var(\hat{f}(x_0))$) and bias ($[Bias(\hat{f}(x_0))]^2$) terms represent reproducible errors, while $Var(\epsilon)$ is the irreducible error. 

Source: Linear Regression Slides

## Question 9
Using the bias-variance tradeoff above, demonstrate that the expected test error is always at least as large as the irreducible error.

The variance term ($Var(\hat{f}(x_0))$) is by-definition non-negative, and the bias term is squared ($[Bias(\hat{f}(x_0))]^2$) guaranteeing that it is also non-negative, therefore, the minimum value that the expected test MSE can take occurs when $Var(\hat{f}(x_0)) = [Bias(\hat{f}(x_0))]^2 = 0$ which yields $E[(y_0 - \hat{f}(x_0))^2] = Var(\epsilon)$, the irreducible error.

Source: An Introduction to Statistical Learning with Applications in R (p32)

## Question 10
Prove the bias-variance tradeoff.

Let $x$ be an unknown test point, $f$ be the true underlying function describing the relationship between $y$ and $x$, and $\epsilon$ be any noise. The mean standard error ($MSE$) of a given function $\hat{f}(x)$ is given by:
$$ E[(y - \hat{f}(x))^2] = E[(f(x) + \epsilon - \hat{f}(x))^2]$$
By multiplying through we get:
$$ E[(f(x) +  \hat{f}(x))^2] + E[\epsilon^2] + 2E[f(x) - \hat{f}(x)] = Var(\epsilon) + E[(f(x) - \hat{f}(x))^2]$$
We can subtract and add the expectation of $\hat{f}(x)$ to the above equation to get:
$$ Var(\epsilon) + E[(E[\hat{f}(x)]-f(x))^2] + E[(\hat{f}(x)-E[\hat{f}(x))^2] - 2E[(f(x) - E[\hat{f}(x)])  *(\hat{f}(x) - E[\hat{f}(x)])]$$

This last term can be rewritten as:
$$-2(f(x) - E[\hat{f}(x)])*(E[\hat{f}(x)]-E[\hat{f}(x)]) = 0$$
Thus we are left with:
$$Var(\epsilon) + E[(E[\hat{f}(x)]-f(x))^2] E[(\hat{f}(x)-E[\hat{f}(x))^2] = Var(\epsilon) + bias[\hat{f}(x)]^2 + var(\hat{f}(x))$$

Source: _Towards Data Science: The Bias-Variance Tradeoff_ 
