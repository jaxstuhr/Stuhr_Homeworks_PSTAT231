---
title: 'Homework #3'
author: "Jaxon Stuhr"
date: "2022-10-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(yardstick)
library(ggthemes)
library(here)
library(corrr)
library(discrim)
library(caret)
library(yardstick)
```

## Classification

For this assignment, we will be working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

```{r}
# set seed for reproducibility
set.seed(147)
# load in titanic data
titanic_raw = read.csv(here("data", "titanic.csv")) %>% 
  mutate(pclass = factor(pclass), 
         survived = factor(survived, levels = c("Yes", "No") ))
```

### Question 1

```{r}
titanic_split <- initial_split(titanic_raw, prop = 0.75,
                                strata = "survived")
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)

summary(titanic_train)

sum(is.na(titanic_train$cabin))

death_prop = sum(titanic_raw$survived == "No")/length(titanic_raw$survived)
```

The training dataset has 516 observations with missing cabin numbers and 124 observations with missing ages that we'll need to account for when modelling these parameters. 

Stratified sampling is necessary due to the fact that the majority of passengers did NOT survive (~62% deceased).

### Question 2

```{r}
titanic_train_summary = titanic_train %>% 
  group_by(survived, age) %>% 
  summarise(count = n())

ggplot() + 
  geom_histogram(data = filter(titanic_train, survived == "Yes"), aes(x = age), bins = 20, fill = "red", alpha = .5) + 
  geom_histogram(data = filter(titanic_train, survived == "No"), aes(x = age), bins = 20, fill = "blue", alpha = .5) 

ggplot() + 
  geom_bar(data = filter(titanic_train, survived == "Yes"), aes(x = pclass), bins = 3, fill = "red", alpha = .5) + 
  geom_bar(data = filter(titanic_train, survived == "No"), aes(x = pclass), bins = 3, fill = "blue", alpha = .5) 

ggplot() + 
  geom_bar(data = filter(titanic_train, survived == "Yes"), aes(x = sex), bins = 2, fill = "red", alpha = .5) + 
  geom_bar(data = filter(titanic_train, survived == "No"), aes(x = sex), bins = 2, fill = "blue", alpha = .5) 

ggplot() + 
  geom_histogram(data = filter(titanic_train, survived == "Yes"), aes(x = fare), bins = 20, fill = "red", alpha = .5) + 
  geom_histogram(data = filter(titanic_train, survived == "No"), aes(x = fare), bins = 20, fill = "blue", alpha = .5) 

```

Based on the above distributions, we see significant correlations between sex and passenger class with whether or not an individual survived, and weaker relationships with age and fare paid.

### Question 3

Using the **training** data set, create a correlation matrix of all continuous variables. Create a visualization of the matrix, and describe any patterns you see. Are any predictors correlated with each other? Which ones, and in which direction?

```{r}
cor_titanic <- titanic_train %>%
  select(passenger_id, pclass, age, sib_sp, parch, fare) %>%
  correlate()
rplot(cor_titanic)
```

From the above correlation plot, we see that passenger class is negatively correlated with fare, which makes sense as higher fares would be associated with 1st and 2nd class passengers. We also see a positive correlation between number of siblings/spouses aboard and number of parents/children aboard, which makes sense as these may likely represent entire families (siblings, spouses, parents, and children) aboard together. 

There are also slight negative correlations between age and passenger class as well as between number of siblings aboard and age, neither of which are surprising. 

### Question 4

```{r}
# generate recipe age as a function of all predictors
titanic_recipe <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = titanic_train) %>% 
  # add imputation for missing ages
  step_impute_linear(age) %>% 
  # dummy vars for nominal predictors
  step_dummy(pclass, sex) %>% 
  # set up interaction terms
  step_interact(terms = ~ starts_with("sex"):fare) %>% 
  step_interact(terms = ~ age:fare) %>%
  # center
  step_center(all_numeric_predictors()) %>%
  # scale
  step_scale(all_numeric_predictors())
```

### Question 5

Logistic Regression (LR) Model + Fit

```{r}
# initialize model
lr_model <- logistic_reg() %>% 
  set_engine("glm")

# set up empty workflow
lr_wflow <- workflow() %>% 
  # add model
  add_model(lr_model) %>% 
  # add recipe
  add_recipe(titanic_recipe)

# apply model to training data
lr_fit <- fit(lr_wflow, titanic_train)
```

### Question 6

Linear Discriminant Analysis (LDA) Model + Fit

```{r}
# initialize model
lda_model <- discrim_linear() %>% 
  set_engine("MASS")

# set up empty workflow
lda_wflow <- workflow() %>% 
  # add model
  add_model(lda_model) %>% 
  # add recipe
  add_recipe(titanic_recipe)

# apply model to training data
lda_fit <- fit(lda_wflow, titanic_train)
```

### Question 7

Quadratic Discriminant Analysis (QDA) Model + Fit

```{r}
# initialize model
qda_model <- discrim_quad() %>% 
  set_engine("MASS")

# set up empty workflow
qda_wflow <- workflow() %>% 
  # add model
  add_model(qda_model) %>% 
  # add recipe
  add_recipe(titanic_recipe)

# apply model to training data
qda_fit <- fit(qda_wflow, titanic_train)
```

### Question 8

Naive-Bayes Model + Fit

```{r}
# initialize model
nb_model <- naive_Bayes() %>% 
  set_engine("klaR")

# set up empty workflow
nb_wflow <- workflow() %>% 
  # add model
  add_model(nb_model) %>% 
  # add recipe
  add_recipe(titanic_recipe)

# apply model to training data
nb_fit <- fit(nb_wflow, titanic_train)
```

### Question 9

Accuracy of all four models

```{r}
lr_acc <- metric_set(accuracy)
# predict logistic regression vals
titanic_lr <- predict(lr_fit, new_data = titanic_train %>% select(-survived))
# add original survival data to new dataset
titanic_lr <- bind_cols(titanic_lr, titanic_train %>% select(survived))
# 
lr_acc(titanic_lr, truth = survived, 
                estimate = .pred_class)
```

```{r}
lda_acc <- metric_set(accuracy)
# predict logistic regression vals
titanic_lda <- predict(lda_fit, new_data = titanic_train %>% select(-survived))
# add original survival data to new dataset
titanic_lda <- bind_cols(titanic_lda, titanic_train %>% select(survived))
# 
lda_acc(titanic_lda, truth = survived, 
                estimate = .pred_class)
```

```{r}
qda_acc <- metric_set(accuracy)
# predict logistic regression vals
titanic_qda <- predict(qda_fit, new_data = titanic_train %>% select(-survived))
# add original survival data to new dataset
titanic_qda <- bind_cols(titanic_qda, titanic_train %>% select(survived))
# 
qda_acc(titanic_qda, truth = survived, 
                estimate = .pred_class)
```

```{r}
nb_acc <- metric_set(accuracy)
# predict logistic regression vals
titanic_nb <- predict(nb_fit, new_data = titanic_train %>% select(-survived))
# add original survival data to new dataset
titanic_nb <- bind_cols(titanic_nb, titanic_train %>% select(survived))
# 
nb_acc(titanic_nb, truth = survived, 
                estimate = .pred_class)
```

The logistic regression model performed best, with an accuracy of 81%. 

### Question 10

Model Performance on Titanic Testing Data

```{r}
lr_acc_testing <- metric_set(accuracy)
# predict logistic regression vals
titanic_lr_testing <- predict(lr_fit, new_data = titanic_test %>% select(-survived))
# add original survival data to new dataset
titanic_lr_testing <- bind_cols(titanic_lr_testing, titanic_test %>% select(survived))
# 
lr_acc_testing(titanic_lr_testing, truth = survived, 
                estimate = .pred_class)
```

The logistic regression model had an accuracy of 81% on the testing data, within .3% of its accuracy on the training data. Any differences are likely simply due to random chance. We do not appear to have overfit the model to the training data or we would have expected it to perform worse on the testing data. 

Confusion Matrix:

```{r}
confusionMatrix(data = titanic_lr_testing$.pred_class, reference = titanic_lr_testing$survived)
```

ROC Curve + Area

```{r}
# calculate survival probabilities from data
titanic_lr_testing_probs <- predict(lr_fit, 
                                    new_data = titanic_test %>% select(-survived),
                                    type = "prob")
# add original survival data to new dataset
titanic_lr_testing_probs <- bind_cols(titanic_lr_testing_probs, 
                                titanic_test %>% select(survived))
# plot roc curve
roc_curve(titanic_lr_testing_probs, .pred_Yes, truth = survived) %>%
  autoplot()
# calculate AUC
titanic_lr_testing_probs %>% 
  roc_auc(truth = survived, .pred_Yes)
```

Above is the plotted ROC Curve, with a calculated AUC of .844

### Required for 231 Students

In a binary classification problem, let $p$ represent the probability of class label $1$, which implies that $1 - p$ represents the probability of class label $0$. The *logistic function* (also called the "inverse logit") is the cumulative distribution function of the logistic distribution, which maps a real number *z* to the open interval $(0, 1)$.

### Question 11


$$
p(z)=\frac{e^z}{1+e^z} = 1- \frac{1}{1+e^z}
$$

$$
=> 1-p = \frac{1}{1-e^z} => 1 + e^z = \frac{1}{1-p} => e^z = \frac{p}{1-p}
$$

$$
=> ln(e^z)=ln\left(\frac{p}{1-p}\right) => z=ln\left(\frac{p}{1-p}\right)
$$

### Question 12

Assume that $z = \beta_0 + \beta_{1}x_{1}$ and $p = logistic(z)$. How do the odds of the outcome change if you increase $x_{1}$ by two? Demonstrate this.

Assume now that $\beta_1$ is negative. What value does $p$ approach as $x_{1}$ approaches $\infty$? What value does $p$ approach as $x_{1}$ approaches $-\infty$?

$$
p(x_{1}) = 1-\frac{1}{1+e^{\beta_0 + \beta_{1}x_{1}}}
$$
$$
p(x_{1}+2) = 1-\frac{1}{1+e^{\beta_0 + \beta_{1}x_{1} + 2\beta_{1}}}
$$

$$
=> \Delta p = p(x_{1}+2) - p(x_{1}) = -\frac{1}{1+e^{\beta_0 + \beta_{1}x_{1} + 2\beta_{1}}} + \frac{1}{1+e^{\beta_0 + \beta_{1}x_{1}}}
$$

$$
 = \frac{1}{1+e^{\beta_0} * e^{\beta_{1}x_{1}}} - \frac{1}{1+e^{\beta_0} * e^{\beta_{1}x_{1}}* e^ {2\beta_{1}}}
$$

$$ 
=(e^{2\beta_{1}} - 1)* \frac{
e^{\beta_0 + \beta_{1}x_{1}}}{
1+e^{\beta_0 + \beta_{1}x_{1}}+e^{\beta_0 + \beta_{1}x_{1}+2\beta_{1}} +e^{2[\beta_0+\beta_{1}(1+x)]}
}
$$

Thus if you increase $x_{1}$ by 2, if $\beta_1 > 0$ then $p$ will increase, if $\beta_1 < 0$ then $p$ will decrease, and if $\beta_1=0$ then $p$ will not change.

Assuming $\beta_1 < 0$, as $x_{1}$ approaches $\infty$ $p$ will approach zero, and as $x_{1}$ approaches $-\infty$ $p$ will approach 1. 