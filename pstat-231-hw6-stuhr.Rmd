---
title: 'Homework #6'
author: "Jaxon Stuhr"
date: "2022-12-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(tidyverse)
library(janitor)
library(tidymodels)
library(glmnet)
library(corrplot)
library(rpart.plot)
library(vip)
library(randomForest)
library(xgboost)
library(ranger)
```

## Excercise 1

Read in data, clean, split, fold, and build recipe,

```{r}
set.seed(7321)
# read in pokemon data, set seed
pokemon_raw = read_csv(here("data", "Pokemon.csv"))
# clean names
pokemon_clean = pokemon_raw %>% 
  clean_names()
pokemon = pokemon_clean %>% 
# filter for only common types
  filter(type_1 %in% c("Bug", "Fire", "Grass", "Normal", "Water", "Psychic")) %>% 
# convert type_1, legendary, generation to factors
  mutate(type_1 = factor(type_1)) %>% 
  mutate(legendary = factor(legendary, levels = c("TRUE", "FALSE"))) %>% 
  mutate(generation = factor(generation)) %>% 
  select(-number, -total)
# split data into training, testing sets
pokemon_split <- initial_split(pokemon, prop = 0.75,
                                strata = "type_1")
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
# fold training set w V=5
pokemon_folds <- vfold_cv(pokemon_train, v = 5, strata = "type_1")
# build recipe
# generate recipe to predict type_1
pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pokemon_train) %>% 
  # dummy vars for nominal predictors
  step_dummy(legendary, generation) %>% 
  # center
  step_center(all_predictors()) %>%
  # scale
  step_scale(all_predictors())
```

## Excercise 2

Correlation matrix of training set.

```{r}
# build correlation matrix of numeric predictors, display as corrplot
# consider generation as numeric predictor
pokemon_cor = cor(pokemon_train %>% mutate(generation = as.numeric(generation)) %>% select(where(is.numeric)))
corrplot(pokemon_cor, method = "number")
```

```{r}
# plot frequencies of categorical predictors to check for correlation
pokemon_categorical = pokemon_train %>% 
  group_by(legendary) %>% 
  summarise(count = n(),
            mean_hp = mean(hp),
            mean_attack = mean(attack),
            mean_defense = mean(defense),
            mean_sp_atk = mean(sp_atk),
            mean_sp_def = mean(sp_def),
            mean_speed = mean(speed))

pokemon_categorical
```

We can observe minor correlations between some numeric predictors, but all <60%, with the highest being sp_def and defense, which makes sense. We additionally see that legendary status is correlated with higher values of all numeric predictors. 

## Excercise 3

Decision Tree model and workflow, tuning cost_complexity. Then autoplot.

```{r}
pokemon_tree_model <- decision_tree() %>%
  set_engine("rpart") %>% 
  set_mode("classification")

pokemon_workflow <- workflow() %>% 
  add_model(pokemon_tree_model %>% set_args(cost_complexity = tune())) %>% 
  add_recipe(pokemon_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  pokemon_workflow, 
  resamples = pokemon_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res)
```

A single decision tree performs best with a cost complexity parameter of ~.01, however, it performs relatively well for all low cost-complexity parameters and then performance drops off for higher ones. 

## Excercise 4

ROC AUC of best performing tree.

```{r}
collect_metrics(tune_res) %>% arrange(mean)
```

The mean ROC AUC of our best performing cost complexity (=.0129) was .65

## Excercist 5a

Fit and visualize best tree with training set.

```{r}
best_complexity <- select_best(tune_res)

pokemon_tree_final <- finalize_workflow(pokemon_workflow, best_complexity)

pokemon_tree_final_fit <- fit(pokemon_tree_final, data = pokemon_train)

pokemon_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

## Excercise 5b

Random forest model and workflow.

```{r}
pokemon_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

pokemon_forest_wf = workflow() %>% 
  add_model(pokemon_forest_model) %>% 
  add_recipe(pokemon_recipe)

param_grid_forest <- grid_regular(mtry(range = c(1, 8)), trees(range = c(10,1000)), min_n(range = c(1,50)), levels = 8)
```

We are tuning parameters _mtry_, _trees_, and _min_n_. 

_mtry_ is the number of predictors that will be sampled at each split when building trees. Our dataset has 8 predictors, so this sets a max on _mtry_, and at least one predictor is needed to split the data, so this sets a min. 

_trees_ is the total number of trees within the random forest model.

_min_n_ is the smallest node size allowable. Nodes will not be split smaller than _min_n_

## Excercise 6

Tune models (5 folds x 8 mtry x 8 trees x 8 min_n = 2560 models) based on ROC AUC.

```{r}
tune_res_forest <- tune_grid(
  pokemon_forest_wf, 
  resamples = pokemon_folds, 
  grid = param_grid_forest, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_forest)
```

The best ROC AUC values were achieved by random forests with more than 1 randomly selected predictors, >40 Trees, and a minimum node size of <30. 

## Excercise 7

ROC AUC of best performing mode.

```{r}
head(collect_metrics(tune_res_forest) %>% arrange(-mean))
```
The best performing model includeded 4 randomly selected predictors, 48 trees, and a min node size of 20. Its mean ROC AUC was .723

## Excercise 8

Variable Importance Plot (VIP) of best performing fit on training set.

```{r}
best_params_forest <- select_best(tune_res_forest)

pokemon_forest_final <- finalize_workflow(pokemon_forest_wf, best_params_forest)

pokemon_forest_final_fit <- fit(pokemon_forest_final, data = pokemon_train)

vip(extract_fit_engine(pokemon_forest_final_fit))
```

sp_atk, speed, and attack were the most important predictors, and the generation dummy variables were least important. I expected that generation would be least important, and didn't have expectations about the others.

## Excercise 9 

Boosted Tree model and workflow, tuning _trees_.

```{r}
pokemon_boost_model <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

pokemon_boost_wf = workflow() %>% 
  add_model(pokemon_boost_model) %>% 
  add_recipe(pokemon_recipe)

param_grid_boost <- grid_regular(trees(range = c(10, 2000)), levels = 10)

tune_res_boost <- tune_grid(
  pokemon_boost_wf, 
  resamples = pokemon_folds, 
  grid = param_grid_boost, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_boost)
```

Model performance peaked around 500 trees, and very slightly declined approaching 2000. 

```{r}
head(collect_metrics(tune_res_boost) %>% arrange(-mean))
```

The best performing model had 452 trees and an ROC AUC of .704

# Excercise 10 

Table of Best-Performing ROC AUC for single pruned tree, random forest, and boosted tree models. Fit the best to the testing dataset.



# Excercise 11

Fit and tune a random forest model to the abalone.txt dataset. Present RMSE on testing data.

```{r}
disp_tbl = matrix(nrow = 3, ncol = 2)
disp_tbl = data.frame(disp_tbl)   
colnames(disp_tbl) = c("Model", "ROC AUC")
disp_tbl[1:3, 1] = c("Pruned Tree", "Random Forest", "Boosted Tree")
disp_tbl[1:3, 2] = c(.65, .72, .70)

disp_tbl
```

The best performing model was the random forest, with an ROC AUC of .72. 

```{r}
best_params_forest <- select_best(tune_res_forest)

pokemon_forest_final <- finalize_workflow(pokemon_forest_wf, best_params_forest)

pokemon_forest_final_fit_testing <- fit(pokemon_forest_final, data = pokemon_train)

class_probs = augment(pokemon_forest_final_fit_testing, new_data = pokemon_test) 
# calculate ROC_AUC
roc_auc = class_probs %>% 
  roc_auc(truth = type_1, .pred_Bug:.pred_Water)

roc_auc
```

Our model had an ROC AUC of.73 on the testing data. 

```{r}
roc_curve(class_probs, .pred_Bug:.pred_Water, truth = type_1) %>%
  autoplot()
```

```{r}
cm = conf_mat(class_probs, estimate = .pred_class, truth = type_1)
# plot heatmap
autoplot(cm, type = "heatmap")
```

Our model was best at predicting Bug, Normal, Fire, and Psychic types, and worse at predicting water, just like the regularization models.

## Excercise 11

Fit and tune a random forest to the abalone.txt data to predict age. What is the RMSE on the testing data?

```{r}
abalone = read_csv(here("data","abalone.csv")) %>%
  mutate(age = rings + 1.5) %>% 
  select(-rings) %>% 
  mutate(type = factor(type))
abalone_split <- initial_split(abalone, prop = 0.75,
                                strata = "age")
abalone_train <- training(abalone_split)
abalone_test <- testing(abalone_split)
# fold training set w V=5
abalone_folds <- vfold_cv(abalone_train, v = 5, strata = "age")
# build recipe
# generate recipe to predict type_1
abalone_recipe <- recipe(age ~ ., data = abalone_train) %>% 
  # dummy vars for nominal predictors
  step_dummy(type) %>% 
  # center
  step_center(all_predictors()) %>%
  # scale
  step_scale(all_predictors())
```

```{r}
abalone_forest_model <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

abalone_forest_wf = workflow() %>% 
  add_model(abalone_forest_model) %>% 
  add_recipe(abalone_recipe)

abalone_param_grid_forest <- grid_regular(mtry(range = c(1, 8)), trees(range = c(10,1000)), min_n(range = c(1,50)), levels = 5)

abalone_tune_res_forest <- tune_grid(
  abalone_forest_wf, 
  resamples = abalone_folds, 
  grid = abalone_param_grid_forest, 
  metrics = metric_set(rmse)
)

autoplot(abalone_tune_res_forest)
```

```{r}
head(collect_metrics(abalone_tune_res_forest) %>% arrange(mean))
```

```{r}
best_params_abalone <- select_best(abalone_tune_res_forest)

abalone_forest_final <- finalize_workflow(abalone_forest_wf, best_params_abalone)

abalone_forest_final_fit <- fit(abalone_forest_final, data = abalone_train)
```

```{r}
abalone_test_results = augment(abalone_forest_final_fit, new_data = abalone_test) 

rmse = abalone_test_results %>% 
  rmse(truth = age, .pred)

rmse
```

The RMSE of the best model (mtry = 5, trees = 292, min_n =	29) on the testing data was 2.1

